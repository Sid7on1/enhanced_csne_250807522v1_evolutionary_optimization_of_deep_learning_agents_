{
  "agent_id": "coder4",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.NE_2508.07522v1_Evolutionary_Optimization_of_Deep_Learning_Agents_",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.NE_2508.07522v1_Evolutionary-Optimization-of-Deep-Learning-Agents- with content analysis. Detected project type: agent (confidence score: 13 matches).",
    "key_algorithms": [
      "Learned",
      "Cma-Es",
      "Hybrid",
      "Policy",
      "Near",
      "Effective",
      "Machine",
      "Discard",
      "Lstm-Based",
      "Per"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.NE_2508.07522v1_Evolutionary-Optimization-of-Deep-Learning-Agents-.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nEvolutionary Optimization of Deep Learning Agents for Sparrow Mahjong\nJim O\u2019Connor, Derin Gezgin, Gary B. Parker\nAutonomous Agent Learning Lab\nConnecticut College\nNew London, Connecticut, USA\n{joconno2, dgezgin, parker }@conncoll.edu\nAbstract\nWe present Evo-Sparrow, a deep learning-based agent for AI\ndecision-making in Sparrow Mahjong, trained by optimizing\nLong Short-Term Memory (LSTM) networks using Covari-\nance Matrix Adaptation Evolution Strategy (CMA-ES). Our\nmodel evaluates board states and optimizes decision policies\nin a non-deterministic, partially observable game environ-\nment. Empirical analysis conducted over a significant number\nof simulations demonstrates that our model outperforms both\nrandom and rule-based agents, and achieves performance\ncomparable to a Proximal Policy Optimization (PPO) base-\nline, indicating strong strategic play and robust policy qual-\nity. By combining deep learning with evolutionary optimiza-\ntion, our approach provides a computationally effective al-\nternative to traditional reinforcement learning and gradient-\nbased optimization methods. This research contributes to the\nbroader field of AI game playing, demonstrating the viability\nof hybrid learning strategies for complex stochastic games.\nThese findings also offer potential applications in adaptive\ndecision-making and strategic AI development beyond Spar-\nrow Mahjong.\n1 Introduction\nGame playing artificial intelligence (AI) has long served as a\nbenchmark and testbed for the continued advancement of the\nfield of AI. From early rule-based systems to modern deep\nreinforcement learning approaches, AI agents have demon-\nstrated superhuman performance in various domains, includ-\ning board games such as chess (Campbell, Hoane, and Hsu\n2002), checkers (Samuel 1959), and Go (Silver et al. 2017b).\nThese advances have been driven by a combination of\nsearch-based techniques, such as minimax with alpha-beta\npruning (Knuth and Moore 1975), Monte Carlo Tree Search\n(MCTS) (Coulom 2006), and deep learning-based policy\noptimization, as exemplified by AlphaGo, AlphaZero, and\nMuZero (Silver et al. 2016, 2017a; Schrittwieser et al. 2019).\nHowever, while deterministic games with perfect informa-\ntion have been well studied, non-deterministic games with\nhidden information and stochastic elements continue to pose\nsignificant challenges (Brown, Sandholm, and Amos 2018).\nThe evolution of AI for game playing has followed a tra-\njectory of increasing complexity, beginning with early sym-\nbolic AI and rule-based systems that relied on handcrafted\nCopyright \u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.strategies. Early successes in this domain were driven by\nalgorithms such as minimax, which effectively modeled\ndecision-making in fully observable, deterministic games\nlike chess and checkers (Shannon 1950). The introduction of\nheuristic evaluations and pruning techniques, such as alpha-\nbeta pruning, enabled AI to search deeper into game trees\nwith reduced computational costs (Knuth and Moore 1975).\nThe next major breakthrough in game playing AI came\nwith probabilistic methods and machine learning-based ap-\nproaches. Temporal Difference (TD) learning, exemplified\nby TD-Gammon (Tesauro 1995), demonstrated the potential\nof reinforcement learning to develop sophisticated strate-\ngies without explicit domain knowledge. This system paved\nthe way for methods that combined reinforcement learn-\ning with tree search, such as MCTS, which was instrumen-\ntal in advancing AI performance in games like Go (Gelly\net al. 2012). MCTS\u2019s success stemmed mainly from its abil-\nity to balance exploration and exploitation efficiently, mak-\ning it well-suited for large decision spaces where exhaustive\nsearch was computationally infeasible.\nDeep learning further revolutionized AI game playing by\nenabling agents to learn directly from structured data. The\nintroduction of deep reinforcement learning, particularly\nwith advancements like Deep Q-Networks (DQN) (Mnih\net al. 2013), showed that neural networks could approxi-\nmate value functions and learn policies for complex envi-\nronments. The combination of deep learning with self-play\nand policy optimization, as demonstrated by AlphaGo and\nits successors AlphaZero and MuZero, achieved superhu-\nman performance in Go, chess, and shogi. These advance-\nments highlighted the power of combining deep neural net-\nworks with reinforcement learning and search techniques,\nsetting new standards for AI in strategic decision-making.\nMahjong, a traditional tile-based game originating in\nChina, represents a complex domain for AI research due\nto its vast state space, elements of randomness, and hid-\nden information. Variants of Mahjong have been studied\nwithin the AI community, with researchers exploring rule-\nbased heuristics, reinforcement learning, and search-based\nstrategies to develop competent agents (Li et al. 2020).\nAmong these variants, a simplified three-player version of\nthe game called Sparrow Mahjong (Koyamada et al. 2023),\nshown in Figure 1, presents an intriguing testbed for AI\ndue to its strategic depth, partial observability, and non-arXiv:2508.07522v1  [cs.NE]  11 Aug 2025\n\n--- Page 2 ---\nFigure 1: An example game state in Sparrow Mahjong, vi-\nsualized in the PGX environment.\ndeterministic nature. Unlike classic four-player Mahjong,\nSparrow Mahjong reduces the number of tiles per player, ac-\ncelerating the decision-making process while retaining core\nstrategic elements.\nTraditional approaches to game playing AI, such as mini-\nmax search, have limited applicability in non-deterministic,\npartially observable environments. Probabilistic reasoning\nand policy learning techniques have been employed to ad-\ndress these challenges in games such as poker (Brown and\nSandholm 2019) and Hanabi (Bard et al. 2020), where bluff-\ning and information inference play crucial roles. In con-\ntrast to these traditional and probabilistic methods, recent\nadvancements in deep learning and Long Short-Term Mem-\nory (LSTM) networks (Hochreiter and Schmidhuber 1997)\nhave enabled AI systems to process sequential dependen-\ncies in decision-making, making these systems well-suited\nfor games that require long-term strategic planning.\nIn this work, we present Evo-Sparrow, a novel approach\nto training an AI agent for Sparrow Mahjong by optimizing\nthe weights of an LSTM model using the Covariance Ma-\ntrix Adaptation Evolution Strategy (CMA-ES) (Ostermeier,\nGawelczyk, and Hansen 1994). Evolutionary strategies such\nas CMA-ES have demonstrated effectiveness in optimizing\nhigh-dimensional neural network policies without requiring\nexplicit gradient computation, making them a compelling al-\nternative to traditional reinforcement learning or gradient-\nbased optimization techniques. Our approach leverages a\nsimple rule-based baseline to provide a structured evaluation\nmetric, allowing us to quantify performance improvements\nand evaluate the efficacy of the model.\n2 Related Work\nMahjong has been a challenging benchmark for artificial in-\ntelligence research, due to its high-dimensional state space,\nstochastic nature, and partial observability (Tang, Chen, and\nWu 2025). According to Chen, Tang, and Wu (2022), the\nstate space complexity of Taiwanese Mahjong is estimated\nto be around 4.3\u00d710185, which exceeds the complexity ofGO ( 10172) and chess ( 1046) (van den Herik, Uiterwijk, and\nvan Rijswijck 2002). Moreover both Li et al. (2024) and Lu,\nLi, and Li (2023) states that the information size of mahjong\nis1048.\nEarly attempts to develop a game-playing agent for\nMahjong have primarily focused on rule-based systems and\nstatistical modeling (Lu, Li, and Li 2023). Monte Carlo\nsimulations and probabilistic opponent modeling have been\nused to predict the state of opponents, winning tiles, game\nscores, and upcoming moves (Mizukami and Tsuruoka\n2015). While these approaches were effective in mimick-\ning human-like strategies, they require pre-set features and\nstruggle in generalization. One notable example of these\ntechniques is SIMCAT, which utilizes a regular Monte\nCarlo approach with heuristic pruning for a Taiwanese vari-\nant of Mahjong. SIMCAT achieved the top placement in\nthe Computer Olympiad 2020 Mahjong Tournament (Chen,\nTang, and Wu 2022).\nMore recent efforts in Mahjong AI have utilized deep\nlearning. For example, a convolutional neural network\n(CNN) based system trained on game records showed im-\nproved discard prediction accuracy compared to the previous\nbaselines (Gao et al. 2019). Additionally, the Suphx system\n(Li et al. 2020) represents a milestone in Mahjong AI, com-\nbining supervised learning with deep reinforcement learning\nand runtime policy adaptation. Suphx includes global reward\nprediction and oracle-guided training to handle the sparsity\nof rewards and the large hidden information in four-player\nJapanese Mahjong. Other work has explored the hierarchi-\ncal decision-making to separate high-level and low-level ac-\ntion choices as well as the fan-backward reward allocation to\nhandle the sparse feedback of Mahjong (Li et al. 2024). Sim-\nilarly, reward variance reduction techniques have been used\nto stabilize training in deep reinforcement learning tasks in\nan environment with randomness in tile draws and high vari-\nance in reward (Li et al. 2022).\nAll these attempts have focused on four-player mahjong\nvariants such as Riichi Mahjong. The Sanma variant of\nMahjong (three-player version of Mahjong) has addition-\nally been addressed by Meowjong (Zhao and Holden 2022),\nwhich uses deep CNNs pretrained with supervised learning\nand fine-tuned by Monte Carlo policy gradient methods. The\nwork done through Meowjong shows that even simple vari-\nants of Mahjong keep the strategic complexity of Mahjong\nand still require strategic planning compared to the four-\nplayer versions. Sparrow Mahjong has not been previously\nstudied in the field of Game AI and there are no publicly\navailable agents or evaluation platforms for it. Our work\naddresses this gap by developing and evaluating a heuris-\ntic rule-based agent, a PPO-based reinforcement learning\nagent, and an evolutionary optimized deep learning agent,\nproviding a comprehensive set of baselines for the Sparrow\nMahjong variant.\nCompared to previous approaches that utilize Monte-\nCarlo simulations, probabilistic modeling, and attention-\nbased classifiers (Truong 2021), our work takes a differ-\nent approach through the combination of Long Short-Term\nMemory (LSTM) networks and Covariance Matrix Adapta-\ntion Evolution Strategy (CMA-ES). We find our approach to\n\n--- Page 3 ---\nbe both computationally efficient and adaptable to sparse-\nreward and partially observable domains such as Sparrow\nMahjong.\n3 Methodology\n3.1 Game State Representation and Input Space\nSparrow Mahjong is a simplified three-player variant of\nMahjong that is characterized by incomplete information\nand stochasticity. Each player begins with 5 tiles and draws\na 6th tile on their turn, after which one tile is discarded. The\nobjective of Sparrow Mahjong is to complete a hand consist-\ning of two melds: either sequences (e.g., 3-4-5 of the same\nsuit) or a combination of triplets (e.g., three 7s) and one pair\n(two identical tiles). Notably, Sparrow Mahjong does not in-\nclude some elements of traditional mahjong such as \u201ccall-\ning tiles\u201d. Additionally, Dora tiles provide bonus points and\nare determined by a revealed indicator; all red dragons are\na Dora, and one tile of each bamboo rank is designated as\nDora at the start of the game.\nThe strategic complexity of Sparrow Mahjong arises from\nseveral key factors in the game. At certain points of the\ngame, players can choose to draw their tiles either from the\nwall, analogous to a deck in other card-based games, or in-\nstead draw a tile that an opponent has just discarded. This\nlayer of interaction makes balancing offensive versus defen-\nsive play a key component of decision-making in the game.\nThe outcome of a Sparrow Mahjong game can be catego-\nrized into four different scenarios: win, draw, lose, and deal-\nin. A player wins by forming a valid hand consisting of two\nmelds (either three consecutive numbers in the same suit or\nthree identical tiles) and a pair of identical tiles. A draw oc-\ncurs when all tiles have been drawn from the middle without\nany player achieving a winning hand. A player is consid-\nered to have \u201ddealt in\u201d when they discard a tile that directly\ncompletes an opponent\u2019s winning hand. Lastly, a player is\nconsidered to have lost when they are not responsible for\nthe victory of the winning player. While a draw and loss do\nnot result in a score change, a win will make the player gain\npoints (relative to the strength of their hand), and a deal-in\nwill make the player lose points to the winning player.\nTo facilitate AI learning, a selection of observable game\ninformation is numerically encoded for input into the LSTM\nnetwork. The representation consists of a 37-dimensional\nvector capturing the player\u2019s hand (6 integers), all players\u2019\ndiscarded tiles (30 integers padded with 0s), and the Dora in-\ndicator (1 integer). To construct the input vector, we used the\nface-values as tile representations which is the default rep-\nresentation of the PGX (Koyamada et al. 2023) library we\nare using to simulate the game. This structure allows us to\nrepresent each tile by its value, and use negative values for\ndora tiles. This representation enables the model to distin-\nguish between tiles without increasing the input dimension-\nality and infer optimal decisions based on historical game\nstates.\n3.2 The LSTM Network\nLong Short-Term Memory Networks (LSTMs) can be used\nfor modeling sequential decision-making tasks, especiallyones that require catching long-term dependencies. Given\nthat the game of Mahjong requires players to track a signifi-\ncant amount of temporal information such as past discards\nand opponents hands while developing long term strate-\ngies, we chose to use an LSTM-based approach for our de-\ncision network. Unlike traditional feedforward neural net-\nworks, LSTMs introduce a recurrent memory mechanism\nthat allows the network to maintain context across multiple\ntime steps. This gives LSTMs an advantage when address-\ning tasks that involve sequential dependencies, such as game\nplaying AI, where past actions and previously seen informa-\ntion play a critical role in future decisions.\nThe core addition of LSTMs is their gated memory ar-\nchitecture, which is built upon three key mechanisms: the\nforget gate, the input gate, and the output gate. The forget\ngate is responsible for determining how much of the past\ninformation should be discarded at each time step. This en-\nsures that irrelevant information does not accumulate in the\nnetwork\u2019s memory, allowing it to focus on the most relevant\naspects of the game state. The input gate manages the ex-\ntent to which new information is stored in the cell state. This\ngate is essential for selecting only the useful inputs, ensuring\nthat the model does not become overloaded with redundant\nor unnecessary features. Lastly, the output gate decides what\nportion of the internal cell state should be passed to the next\nlayer, which allows the model to adjust its decision-making\nprocess based on its understanding of the game environment.\nThe LSTM network used in this project consists of three\nLSTM layers followed by eight fully connected layers with\n32 neurons. The LSTM layers are used for processing his-\ntorical game states, and extracting important features from\npast moves. These features include observed opponent ac-\ntions and the overall state of the game at a given point in\ntime. By passing sequential game states through multiple\nLSTM layers, the network is able to encode long-term pat-\nterns that can used in the decision-making. These patterns\ncan be processed in fully connected layers, which can gen-\nerate a decision on which tile to discard. The fully connected\nlayers act as a high-level mechanism to map learned features\nto actions. At each decision step, the output layer selects the\ntile to discard. The network structure is shown in Figure 2.\nThe LSTM architecture was chosen via early pilot experi-\nments with different architectures, and the final architecture\nwas found to be the best performing in these early test. Im-\nportantly, we aimed to keep the network size manageable to\nensure the efficacy of the normal variant of CMA-ES, as the\ncovariance matrix scales quadratically with the number of\nparameters which can lead to excessive memory usage.\nAn important advantage of LSTMs over traditional Re-\ncurrent Neural Networks (RNNs) is their ability to address\nthe vanishing gradient problem, which happens during the\ntraining of deep models. In standard RNNs, gradients tend\nto disappear exponentially as they are propagated through\nlong sequences, making it difficult for the network to learn\ndependencies that span multiple time steps. LSTMs address\nthis issue with their memory cell structure which maintains\ninformation across extended time steps.\nTo optimize the performance of the LSTM, batch and\ndropout regularization were applied to prevent overfitting\n\n--- Page 4 ---\nFigure 2: A high-level model of our LSTM network structure.\nand enhance generalization. Batch normalization standard-\nizes the input activations at each layer, ensuring that model\nthe remains stable and learns efficiently. Dropout regulariza-\ntion prevents the model from overfitting on specific features\nand encourages the model for more distributed representa-\ntions, leading to a better generalization across different sce-\nnarios.\n3.3 CMA-ES Optimization\nThe Covariance Matrix Adaptation Evolution Strategy\n(CMA-ES) is a derivative-free optimization algorithm used\nfor solving high-dimensional, non-linear optimization prob-\nlems. Unlike traditional gradient-based optimization meth-\nods, CMA-ES maintains a population of candidate solutions\nand iteratively updates the search distribution. The optimiza-\ntion process models candidate solutions as samples from a\nmultivariate normal distribution. At each iteration, CMA-\nES generates a population of weight configurations, eval-\nuates them based on gameplay performance, and updates\nthe search distribution parameters. The global step size ( \u03c3)\nis dynamically adjusted via cumulative step-size adaptation\n(CSA) to balance exploration and exploitation. We use the\nstandard CMA-ES variant with full covariance matrix adap-\ntation, allowing the algorithm to model complex parame-\nter correlations during the search. The search is initialized\nwith a randomly sampled population and an initial step size\nof\u03c3= 1. CMA-ES optimizes all \u224834,000 weights of\nthe LSTM network, where each solution represents a set of\nweights. Each candidate solution was evaluated 200 times\nagainst two rule-based expert agents. The cumulative score\nacross these 200 games was recorded as the fitness value of\nthe candidate solution. These fitness values were fed back\ninto the CMA-ES algorithm, which updates the search dis-\ntribution.3.4 Training and Evaluation of Evo-Sparrow\nThe evaluation of each candidate solution sampled from\nCMA-ES was achieved by playing 200 games against two\nexpert agents. 200 games were chosen to balance the sta-\ntistical significance and keep the training time within a rea-\nsonable range. To eliminate a potential bias, the player order\nwas randomized for each game to ensure there was no ad-\nvantage or disadvantage due to the player ordering. At the\nend of 200 games, a fitness value was calculated for each\ncandidate solution by summing the scores of the candidate\nsolution for all 200 games.\nAfter 50 generations of training, the best solution among\nthe final set of candidate solutions is picked. To evaluate this\nbest solution, 1,000,000 games were played against PPO-\noptimized and rule-based agents. 1,000,000 was chosen to\nreduce the variance in the evaluation and ensure statistical\nsignificance. Considering the stochastic nature of Sparrow\nMahjong, a high number of trials is required to ensure sta-\ntistical significance. The results from this evaluation are pre-\nsented and discussed in Section 4.\n3.5 Rule-Based Expert Agent\nTo benchmark performance and serve as a training oppo-\nnent, a rule-based agent was developed implementing fun-\ndamental Sparrow Mahjong heuristics. The agent prioritizes\ntile retention based on sequence potential, avoids breaking\nexisting sets, and minimizes discards that benefit opponents.\nThis structured heuristic-based approach provides a robust\nbaseline for comparison against Evo-Sparrow.\nThe agent\u2019s ranking system assigns a priority value to\neach tile in the player\u2019s hand. This system follows three\nprinciples. First, it prioritizes high-value tiles, as they con-\ntribute more to the player\u2019s score. Second, it avoids discard-\ning tiles that are already part of a set or a pair. This ensures\nthat the agent never breaks down a set that is already con-\nstructed. Lastly, the system favors keeping tiles that have a\nhigher set-forming potential. For example, the agent avoids\n\n--- Page 5 ---\nkeeping terminal tiles (1 and 9), which have a lower po-\ntential of forming a sequence. On the other hand, it favors\ntiles more towards the center of the distribution which have\na high potential for forming a set. Once all of the tiles are\nevaluated, the tile with the highest discard priority score is\ndiscarded. In cases where multiple tiles receive the same\npriority score, the agent selects a discard randomly from\nthe highest-priority candidates. The agent does not perform\nprobabilistic modeling, but follows a fixed heuristic strategy\nas a straightforward and simple baseline.\n3.6 Proximal Policy Optimization Agent\nTo establish a strong reinforcement learning baseline, we\nhave implemented an agent (PPO-Sparrow) that uses Prox-\nimal Policy Optimization (PPO), a widely-used and state-\nof-the-art policy gradient method for sequential decision-\nmaking tasks (Schulman et al. 2017). PPO has gained signif-\nicant attention due to its robust performance and computa-\ntional efficiency, particularly for environments characterized\nby partial observability and stochasticity, such as Sparrow\nMahjong (Azizzadenesheli, Yue, and Anandkumar 2018).\nPPO-Sparrow uses an LSTM-based neural network archi-\ntecture that is identical to the architecture of Evo-Sparrow\nto ensure a fair and meaningful comparison. We made this\ndecision to isolate the effect of the optimization method and\nensure a fair comparison. The network consists of 3 LSTM\nlayers followed by 8 fully-connected layers, each containing\n32 neurons. The network outputs two distinct predictions:\na policy head that generates a probability distribution over\npossible actions using a softmax layer, and a value head that\nestimates the expected cumulative reward from the current\nstate.\nSimilar to Evo-Sparrow, each training iteration consisted\nof 200 parallel games, enabling efficient data collection for\nPPO updates. Each policy update involved performing 4\nepochs of optimization per collected experience batch. To\nensure stable training and avoid excessive divergence be-\ntween policy updates, we applied clipping to both the pol-\nicy and the value-function updates. Specifically, we used\na PPO clipping parameter ( \u03f5) of 0.2, along with a learn-\ning rate of 10\u22124, a discount factor ( \u03b3) of 0.99, and entropy\nregularization to encourage exploration and prevent prema-\nture convergence. Advantage estimates were computed us-\ning Generalized Advantage Estimation (GAE) and subse-\nquently normalized to further stabilize training. Addition-\nally, we clipped gradients to prevent potential high-variance\nupdates.\nPPO-Sparrow was trained for 1,750generations (PPO up-\ndates) to match the exact number of games played by the\nEvo-Sparrow agent during training, totaling 350,000games.\nThis setup allows for a direct comparison of learning effi-\nciency and performance between two methods. Detailed re-\nsults of PPO-Sparrow are provided in Section 4.\n4 Results\n4.1 Rule-Based and Random Agent Performance\nBefore evaluating Evo-Sparrow and PPO-Sparrow, we es-\ntablished performance benchmarks for random and rule-Table 1: Performance of Random and Rule-Based agents in\ndifferent match-ups over 1,000,000games per configuration.\nAvg. Score Win % Draw % Loss % Deal-in %\nRandom 0.0023 8.19 75.52 5.79 10.50\nRandom \u20130.0018 8.15 75.52 5.82 10.51\nRandom \u20130.0006 8.17 75.52 5.79 10.52\nRule-Based 1.4342 23.83 61.28 5.38 9.51\nRandom \u20130.7120 7.49 61.28 12.37 18.87\nRandom \u20130.7223 7.49 61.28 12.27 18.96\nRule-Based 0.0045 19.65 41.52 15.68 23.15\nRule-Based \u20130.0006 19.63 41.52 15.70 23.15\nRule-Based \u20130.0039 19.59 41.52 15.67 23.22\nbased agents.\nTable 1 presents results over 1,000,000 games, detail-\ning average score, win rate, draw rate, loss rate, and deal-\nin rate. These results demonstrate the clear gap in perfor-\nmance between our random and rule-based agents. When all\nthree agents were random, the average scores remained very\nclose to zero, with nearly equal win, loss, and deal-in ratios.\nWhen a single rule-based agent was matched with two ran-\ndom agents, the rule-based agent significantly outperformed\nthe random opponents with an average score of 1.4342 and\na win rate of 23.83% compared to the 7.49% win rate of\nthe random agents. This result confirms that our heuristic-\nbased tile selection provides a clear advantage over a ran-\ndom agent and can serve as a reliable opponent for training\nEvo-Sparrow and PPO-Sparrow.\n4.2 Evo-Sparrow Performance\nEvo-Sparrow was evaluated against both Random and Rule-\nBased agents across 1,000,000 games in several mixed-\nagent configurations. As shown in Table 2, Evo-Sparrow\nconsistently outperforms the baseline agents across all\nmatch-ups.\nIn a setting with one Evo-Sparrow agent, one Rule-Based\nagent, and one Random agent, Evo-Sparrow achieved a win\nrate of 28.55% and an average score of 0.8687, outperform-\ning both the Rule-Based (20.91% win rate) and Random\n(6.64% win rate) agents. Evo-Sparrow also maintained a\nlower deal-in rate, indicating stronger defensive play along-\nside its offensive capability. Against two Rule-Based agents,\nEvo-Sparrow again led with a win rate of 26.39% compared\nto the Rule-Based agents\u2019 win rates of 18.70% and 18.71%.\nAdditionally, the average deal-in rate of Evo-Sparrow re-\nmained lower, showing better tile discard decisions that does\nnot help its opponents.\nIn order to evaluate the stability and consistency of the\nlearned policy, we conducted a self-play evaluation using\nthree identical instances of the final Evo-Sparrow agent. Ta-\nble 2 shows that each agent achieved nearly identical per-\nformance in all metrics. This equivalence shows that the\nevolved agents do not rely on exploiting the predictable be-\nhavior of the weaker rule-based agent and demonstrate con-\nsistent performance against agents with equal strength.\n\n--- Page 6 ---\nTable 2: Performance of Evo-Sparrow in mixed match-up\nand self-play over 1,000,000games per configuration.\nAvg. Score Win % Draw % Loss % Deal-in %\nEvo-Sparrow 0.8687 28.55 44.17 10.97 16.31\nRule-Based 0.5051 20.91 44.17 12.65 22.28\nRandom \u20131.3738 6.64 44.17 18.66 30.54\nEvo-Sparrow 0.2648 26.39 36.65 14.68 22.28\nRule-Based \u20130.1270 18.70 36.65 17.15 27.50\nRule-Based \u20130.1378 18.71 36.65 17.02 27.62\nEvo-Sparrow \u20130.0027 19.20 42.81 10.79 27.20\nEvo-Sparrow 0.0062 19.25 42.81 10.77 27.17\nEvo-Sparrow \u20130.0035 19.17 42.81 10.75 27.27\nTable 3: Direct comparison of Evo-Sparrow, PPO-Sparrow,\nand Rule-Based agents in a match-up over 1,000,000games.\nAvg. Score Win % Draw % Loss % Deal-in %\nEvo-Sparrow 0.1934 22.80 36.08 17.60 23.52\nPPO-Sparrow 0.1868 22.62 36.08 17.74 23.57\nRule-Based \u20130.3802 19.07 36.08 9.98 34.87\n4.3 Comparative Evaluation with PPO-Sparrow\nTo assess the competitiveness of Evo-Sparrow against a\nwidely used reinforcement learning baseline, we imple-\nmented PPO-Sparrow, an agent trained using Proximal Pol-\nicy Optimization (PPO) with the same LSTM-based archi-\ntecture as Evo-Sparrow. This comparison allows us to isolate\nthe effects of the training methodology and directly compare\nthe optimizers.\nTable 3 presents the results of a match-up where Evo-\nSparrow, PPO-Sparrow, and a Rule-Based agent were evalu-\nated over 1,000,000games. Evo-Sparrow achieved a slightly\nhigher win rate (22.80%) and average score (0.1934) com-\npared to PPO-Sparrow (22.62% win rate, 0.1868 score),\nwhile also maintaining a marginally lower deal-in rate\n(23.52% vs. 23.57%). These results\u2018 demonstrate that Evo-\nSparrow is able to match or exceed the performance of\na reinforcement learning based agent trained for an equal\nnumber of games. The rule-based agent in this match-\nup achieved the lowest win rate (19.07%) and a signifi-\ncantly negative average score (\u20130.3802), indicating that both\nlearned agents effectively outperform handcrafted heuris-\ntics. Statistical tests show that while the score difference\nbetween Evo-Sparrow and PPO-Sparrow is not significant\nbased on a t-test ( p-value = 0.1721 ), a chi-square-test on\nwin-rates confirms a statistically significant advantage for\nEvo-Sparrow ( p-value = 0.0001 ).\nIn addition to performance metrics, we also compared the\ntraining efficiency of both approaches. Across 10 indepen-\ndent training runs, PPO-Sparrow required an average of ap-\nproximately 106 minutes and 43 seconds to complete train-\ning, whereas Evo-Sparrow completed training in an aver-\nage of 40 minutes and 19 seconds. This difference is pri-\nmarily due to the inherent ability of CMA-ES to paral-\nlelize, which allows individuals to be distributed across mul-\nFigure 3: Median and 25th - 75th percentile fitness progres-\nsion for 100 independent training runs\ntiple processes. In contrast, PPO relies on sequential policy\nupdates and gradient-based optimization, which are inher-\nently more difficult to parallelize. As a result, Evo-Sparrow\nachieves competitive performance with significantly lower\ntraining time, making it a computationally efficient alterna-\ntive for learning in high-variance, partially observable envi-\nronments.\n4.4 Consistency of Evo-Sparrow Training\nTo further validate the consistency and robustness of Evo-\nSparrow, we conducted 100 independent training runs. In\neach run, the agent was trained for 50 generations with a\npopulation of 35 candidate solutions per generation, and ev-\nery candidate solution was evaluated by playing 200 games\nper solution. Figure 3 illustrates the progression of total fit-\nness across 50 generations over 100 independent training tri-\nals. The median fitness line demonstrates a fast improvement\nin the initial generations, climbing drastically from -9,000\nat generation 1 to stable positive fitness levels by genera-\ntion 20. Beyond generation 20, fitness continues to rise at a\nreduced rate, stabilizing near generation 40. The narrowing\n25-75th percentile range shows that the variability among\ncandidate solutions decreases as training proceeds. These re-\nsults show the reliability and consistency of our approach\nacross independent training runs.\n5 Limitations\nWhile our implementation shows promising results in Spar-\nrow Mahjong, there are several limitations that constrain the\ngeneralization of this work.\nOur use of CMA-ES for optimizing the LSTM network\nachieves strong performance without requiring gradient in-\nformation or reward shaping. CMA-ES is simpler and less\nGPU-intensive compared to traditional reinforcement learn-\ning methods. Nonetheless, the training still requires signifi-\ncant computational cost due to the need for repeated eval-\nuations of candidate solutions across generations. More-\nover, the covariance matrix updates can become memory-\n\n--- Page 7 ---\nintensive in high-dimensional parameter spaces. Although\nthe computation and memory costs are moderate relative to\nlarge reinforcement learning pipelines, these costs still limit\nthe scalability to larger and more complex models.\nAnother limitation of this study is related to the avail-\nable baselines. As Sparrow Mahjong has not been previously\nexplored thoroughly, there are no publicly available agents\nor standardized benchmarks. During the training phase, we\nused a handcrafted rule-based agent as a baseline, which\ndoes not reflect the full capabilities of learning or search-\nbased approaches. To address this gap, we also implemented\na reinforcement learning agent using Proximal Policy Opti-\nmization (PPO), providing a stronger and more informative\nbaseline for comparison against our evolutionary method. It\nremains possible to explore even more complex and larger\nneural architectures or to incorporate opponent modeling,\nhierarchical reasoning, or attention mechanisms, which may\nresult in stronger competitors in future work.\nDespite these limitations, our work demonstrates that evo-\nlutionary optimization of sequential models is an effective\napproach for decision-making in stochastic and partially ob-\nservable games. The combination of CMA-ES and LSTM\ncreates a robust learning framework that avoids the com-\nplexity of gradient-based methods like reinforcement learn-\ning while producing agents that outperform both random and\nheuristic baselines. While further enhancements are needed\nto generalize this approach to more complex domains, the\ncurrent results provide a strong foundation for future re-\nsearch into hybrid optimization strategies for game AI.\n6 Conclusion\nThis study demonstrates the effectiveness of combining\nLSTM networks with CMA-ES for decision-making in\nstochastic, partially observable environments. Evo-Sparrow\nsignificantly outperforms heuristic-based approaches and\ndemonstrates competitive performance when compared to a\nPPO-based reinforcement learning agent. The results sug-\ngest that hybrid learning strategies can adapt well to dy-\nnamic game states, balancing offensive and defensive strate-\ngies while leveraging long-term dependencies in gameplay.\nFuture work can explore refinements such as incorpo-\nrating opponent modeling techniques to enhance predictive\naccuracy and adaptability. Furthermore, expanding the ar-\nchitecture to handle more complex decision trees or neural\narchitectures could further improve strategic performance.\nBeyond Sparrow Mahjong, the principles outlined in this\nstudy could be applied to other domains involving prob-\nabilistic decision-making in uncertain or hidden informa-\ntion environments. Alternative state representations, includ-\ning richer encodings of tile relationships or attention-based\nmechanisms, can further enhance strategic decision-making.\nFinally, while this work focuses on Sparrow Mahjong, the\nproposed methodology can be extended to other stochastic,\nhidden-information games, and more broadly to real-world\ndomains requiring probabilistic reasoning under uncertainty.References\nAzizzadenesheli, K.; Yue, Y .; and Anandkumar, A.\n2018. Policy gradient in partially observable environ-\nments: Approximation and convergence. arXiv preprint\narXiv:1810.07900 .\nBard, N.; Foerster, J. N.; Chandar, S.; Burch, N.; Lanctot,\nM.; Song, H. F.; Parisotto, E.; Dumoulin, V .; Moitra, S.;\nHughes, E.; Dunning, I.; Mourad, S.; Larochelle, H.; Belle-\nmare, M. G.; and Bowling, M. 2020. The Hanabi challenge:\nA new frontier for AI research. Artif. Intell. , 280(C).\nBrown, N.; and Sandholm, T. 2019. Superhuman AI for mul-\ntiplayer poker. Science , 365: 885 \u2013 890.\nBrown, N.; Sandholm, T.; and Amos, B. 2018. Depth-\nlimited solving for imperfect-information games. In Pro-\nceedings of the 32nd International Conference on Neural\nInformation Processing Systems , NIPS\u201918, 7674\u20137685. Red\nHook, NY , USA: Curran Associates Inc.\nCampbell, M.; Hoane, A. J.; and Hsu, F.-h. 2002. Deep Blue.\nArtif. Intell. , 134(1\u20132): 57\u201383.\nChen, J.; Tang, S.; and Wu, I. 2022. Monte-Carlo Simulation\nfor Mahjong. Journal of Information Science and Engineer-\ning, 38(4): 775\u2013790.\nCoulom, R. 2006. Efficient Selectivity and Backup Opera-\ntors in Monte-Carlo Tree Search. In Computers and Games ,\nvolume 4630, 72\u201383. Springer. ISBN 978-3-540-75537-1.\nGao, S.; Okuya, F.; Kawahara, Y .; and Tsuruoka, Y . 2019.\nBuilding a Computer Mahjong Player via Deep Convolu-\ntional Neural Networks. ArXiv , abs/1906.02146.\nGelly, S.; Kocsis, L.; Schoenauer, M.; Sebag, M.; Silver, D.;\nSzepesv \u00b4ari, C.; and Teytaud, O. 2012. The grand challenge\nof computer Go: Monte Carlo tree search and extensions.\nCommun. ACM , 55(3): 106\u2013113.\nHochreiter, S.; and Schmidhuber, J. 1997. Long Short-Term\nMemory. Neural Comput. , 9(8): 1735\u20131780.\nKnuth, D. E.; and Moore, R. W. 1975. An Analysis of\nAlpha-Beta Pruning. Artificial Intelligence , 6: 293\u2013326.\nKoyamada, S.; Okano, S.; Nishimori, S.; Murata, Y .; Habara,\nK.; Kita, H.; and Ishii, S. 2023. Pgx: Hardware-Accelerated\nParallel Game Simulators for Reinforcement Learning. In\nAdvances in Neural Information Processing Systems , vol-\nume 36, 45716\u201345743. Curran Associates, Inc.\nLi, J.; Koyamada, S.; Ye, Q.; Liu, G.; Wang, C.; Yang, R.;\nZhao, L.; Qin, T.; Liu, T.; and Hon, H. 2020. Suphx: Mas-\ntering Mahjong with Deep Reinforcement Learning. CoRR ,\nabs/2003.13590.\nLi, J.; Wu, S.; Fu, H.; Fu, Q.; Zhao, E.; and Xing, J. 2022.\nSpeedup Training Artificial Intelligence for Mahjong via\nReward Variance Reduction. In 2022 IEEE Conference on\nGames (CoG) , 345\u2013352.\nLi, X.; Liu, B.; Wei, Z.; Wang, Z.; and Wu, L. 2024. Tjong:\nA transformer-based Mahjong AI via hierarchical decision-\nmaking and fan backward. CAAI Transactions on Intelli-\ngence Technology , 9(4): 982\u2013995.\nLu, Y .; Li, W.; and Li, W. 2023. Official International\nMahjong: A New Playground for AI Research. Algorithms ,\n16: 235.\n\n--- Page 8 ---\nMizukami, N.; and Tsuruoka, Y . 2015. Building a computer\nMahjong player based on Monte Carlo simulation and oppo-\nnent models. In 2015 IEEE Conference on Computational\nIntelligence and Games (CIG) , 275\u2013283.\nMnih, V .; Kavukcuoglu, K.; Silver, D.; Graves, A.;\nAntonoglou, I.; Wierstra, D.; and Riedmiller, M. A. 2013.\nPlaying Atari with Deep Reinforcement Learning. CoRR ,\nabs/1312.5602.\nOstermeier, A.; Gawelczyk, A.; and Hansen, N. 1994. A\nDerandomized Approach to Self-Adaptation of Evolution\nStrategies. Evolutionary Computation , 2(4): 369\u2013380.\nSamuel, A. L. 1959. Some Studies in Machine Learning\nUsing the Game of Checkers. IBM Journal of Research and\nDevelopment , 3(3): 210\u2013229.\nSchrittwieser, J.; Antonoglou, I.; Hubert, T.; Simonyan, K.;\nSifre, L.; Schmitt, S.; Guez, A.; Lockhart, E.; Hassabis, D.;\nGraepel, T.; Lillicrap, T. P.; and Silver, D. 2019. Master-\ning Atari, Go, Chess and Shogi by Planning with a Learned\nModel. CoRR , abs/1911.08265.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347 .\nShannon, C. E. 1950. Programming a Computer for Playing\nChess. Philosophical Magazine , 41: 256\u2013275.\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;\nVan Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;\nPanneershelvam, V .; Lanctot, M.; et al. 2016. Mastering the\ngame of Go with deep neural networks and tree search. Na-\nture, 529(7587): 484\u2013489.\nSilver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai,\nM.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel,\nT.; Lillicrap, T. P.; Simonyan, K.; and Hassabis, D. 2017a.\nMastering Chess and Shogi by Self-Play with a General Re-\ninforcement Learning Algorithm. CoRR , abs/1712.01815.\nSilver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.;\nHuang, A.; Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton,\nA.; Chen, Y .; Lillicrap, T.; Hui, F.; Sifre, L.; van den Driess-\nche, G.; Graepel, T.; and Hassabis, D. 2017b. Mastering the\ngame of Go without human knowledge. Nature , 550: 354\u2013.\nTang, S.-C.; Chen, J.-C.; and Wu, I.-C. 2025. An Efficient\nMethod for Assessing the Strength of Mahjong Programs. In\nProceedings of the 17th International Conference on Agents\nand Artificial Intelligence - Volume 2: ICAART , 124\u2013132.\nINSTICC, SciTePress. ISBN 978-989-758-737-5.\nTesauro, G. 1995. Temporal difference learning and TD-\nGammon. Commun. ACM , 38(3): 58\u201368.\nTruong, T.-D. 2021. A Supervised Attention-Based Multi-\nclass Classifier for Tile Discarding in Japanese Mahjong .\nMaster\u2019s thesis, University of Agder, Grimstad, Norway.\nvan den Herik, H.; Uiterwijk, J. W.; and van Rijswijck, J.\n2002. Games solved: Now and in the future. Artificial Intel-\nligence .\nZhao, X.; and Holden, S. 2022. Building a 3-Player\nMahjong AI using Deep Reinforcement Learning. ArXiv ,\nabs/2202.12847.",
  "project_dir": "artifacts/projects/enhanced_cs.NE_2508.07522v1_Evolutionary_Optimization_of_Deep_Learning_Agents_",
  "communication_dir": "artifacts/projects/enhanced_cs.NE_2508.07522v1_Evolutionary_Optimization_of_Deep_Learning_Agents_/.agent_comm",
  "assigned_at": "2025-08-12T19:32:44.984543",
  "status": "assigned"
}